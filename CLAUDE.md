# Cupid AI - Technical Context

> Current implementation details and architecture. See PLAN.md for planned features.

## Architecture Overview

**Stack**: React + Vite frontend, Node.js + Express backend, SQLite database
**AI Provider**: OpenRouter (configurable per-user LLM settings)
**Storage**: IndexedDB (frontend) + SQLite (backend)

## Key Implementation Details

### Frontend Structure
- **Main Layout**: `MainLayout.jsx` - Sidebar with matches, unread indicators, nav
- **Core Pages**:
  - `Home.jsx` - Tinder-style swipe interface
  - `Chat.jsx` - Individual chat conversations
  - `Library.jsx` - Character management
  - `Profile.jsx` - User settings and LLM config
- **Services**:
  - `characterService.js` - IndexedDB operations for characters
  - `chatService.js` - Backend API calls for conversations/messages
  - `api.js` - Axios instance with auth token injection

### Backend Structure
- **Routes**: `auth.js`, `users.js`, `characters.js`, `chat.js`
- **Services**:
  - `aiService.js` - OpenRouter integration, system prompts, token counting/trimming, decision engine
  - `engagementService.js` - Character state tracking, engagement windows, status-based delays
- **Utils**: `logger.js` - File logging with console intercept (auto-clears on restart)
- **Database**: `database.js` - better-sqlite3 with auto-migrations
- **Auth**: JWT tokens, `authenticateToken` middleware

## Important Patterns

### Character Data Flow
1. PNG upload → Parse v2 card → Store in IndexedDB (frontend)
2. Like character → Sync to backend SQLite (user_id + character_id)
3. Dating profile generated on-demand via OpenRouter (cached in cardData)

### Chat System
- Conversations created on first message
- `unread_count` tracks unread messages per conversation
- Messages marked as read when chat is opened
- AI responses use character description + dating profile + system prompt
- Component unmount tracking prevents stale state updates
- **Context window management**: Token-based trimming keeps recent messages, drops old ones
- **Multi-message system**: Newlines split into separate bubbles with 800ms delays
- **Typing indicator**: Text-based "{name} is typing..." with random 500-2000ms delay
- **Message animations**: Slide-up on new messages only (tracked via `newMessageIds` Set)
- **Dual LLM system**: Decision LLM makes behavioral decisions (reactions), Content LLM generates responses
- **Reaction system**: Emoji reactions appear rarely (1 in 5 messages) on emotionally significant user messages
- **Message editing/deletion**: Users can edit or delete messages, delete-from removes message + all after it
- **Regenerate**: Users can regenerate the last AI response
- **Schedule system**: Characters have weekly schedules (online/away/busy/offline) generated by LLM
- **Status display**: Chat header shows real-time status with colored badges and activity text
- **Engagement windows**: Realistic response timing with initial delays and fast conversation bursts (2-5 messages)

### Unread Notifications
- Backend increments `unread_count` on AI responses
- `markAsRead` endpoint resets count when user views chat
- `characterUpdated` custom event triggers sidebar refresh
- First messages generated at match time (50/50 chance)

## Critical Bugs Fixed

### Race Condition with Chat Navigation
**Issue**: Sending message in Chat A, navigating to Chat B before response → Chat B shows Chat A's messages
**Fix**: Capture `characterId` at request start, check `isMountedRef.current` before state updates

### Double First Message Generation
**Issue**: First message generated twice due to React strict mode + useEffect
**Fix**: Moved generation to swipe action (Home.jsx), removed from Chat.jsx load

### Unread Count Not Clearing
**Issue**: Messages marked as read even when user navigated away
**Fix**: Added `isMountedRef` to track if user is still viewing that chat

## Database Schema Notes

### Key Tables
- `users` - LLM settings (model, temperature, max_tokens, context_window, etc.) + Decision LLM settings
- `conversations` - Links user + character, tracks `unread_count`, `last_message`
- `messages` - role ('user'|'assistant'), content, timestamps, reaction (emoji or null)
- `characters` - Synced from IndexedDB, stores full card_data JSON, schedule_data, schedule_generated_at
- `character_states` - Per user-character engagement tracking (status, engagement_state, messages_remaining, last_check_time)

### Migrations
Auto-run on startup via `database.js`:
- Adds LLM settings columns if missing
- Adds `unread_count` column if missing
- Adds `llm_context_window` column (default 4000 tokens)
- Adds Decision LLM settings columns (model, temperature, max_tokens, context_window with lower defaults)
- Adds `reaction` column to messages table
- Adds `schedule_data` and `schedule_generated_at` columns to characters table
- Creates `character_states` table for engagement tracking

## AI System

### Dual LLM Architecture
- **Decision Engine** (small LLM): Analyzes conversation context and makes behavioral decisions
  - Determines if character should react with emoji (rare, 1 in 5 messages or less)
  - Returns: `{ reaction: "emoji"|null, shouldRespond: boolean }`
  - Uses separate user-configurable settings (lower temperature, fewer tokens)
  - Located in `aiService.js` `makeDecision()`
- **Content Generator** (large LLM): Creates actual character responses
  - Uses full character prompt with dating app style enforcement
  - Token-based context window management
  - Located in `aiService.js` `createChatCompletion()`

### Content LLM System Prompt Rules
Located in `aiService.js` `buildSystemPrompt()`:
- Explicitly forbids roleplay formatting (*actions*, "dialogue")
- Enforces dating app text message style
- Uses character description + scenario + system_prompt fields
- Dating profile injected into character data if available

### Message Flow
1. User sends message → saved to DB
2. Check character's schedule → determine current status (online/away/busy/offline)
3. Get/create engagement state → calculate response delay
4. If disengaged, 70% chance to start engagement window (2-5 fast messages)
5. Apply calculated delay (status-based or engagement-based)
6. Decision LLM analyzes context → decides reaction
7. Content LLM generates response (if `shouldRespond: true`)
8. Response saved with reaction → engagement message consumed
9. Response displayed with reaction badge on user's message

### Engagement Window System
Located in `engagementService.js`:
- **Status-based delays**:
  - Online: 30-120 seconds
  - Away: 5-20 minutes (300-1200s)
  - Busy: 15-60 minutes (900-3600s)
  - Offline: No response
- **Engagement bursts**: When character "engages", they respond quickly for 2-5 messages (5-15s delays)
- **70% engagement probability**: On first message when disengaged
- **State tracking**: character_states table tracks engagement per user-character pair
- **Automatic consumption**: Each response consumes one engagement message until burst depletes

## Development Commands

```bash
# Frontend
cd frontend && npm run dev  # Port 5173

# Backend
cd backend && npm start     # Port 3000 (nodemon auto-restart)
```

## Known Quirks

- Character names default to "Character" when syncing to backend (cosmetic)
- Dating profile generation uses fixed prompt in `characters.js`
- Image parsing happens client-side, backend only stores metadata
- Swipe undo limited to last action (no full history)
- Match animation doesn't auto-dismiss (user must click)

## Recent Changes

- **Phase 2: Schedule & Engagement System** ✅
  - LLM-generated weekly schedules (online/away/busy/offline status blocks)
  - Real-time status calculation from schedule + current time
  - Status display in chat header with colored badges
  - Engagement window system: Initial delays based on status, then 2-5 message bursts with fast responses (5-15s)
  - character_states table tracks engagement per user-character pair
  - 70% engagement probability on first message when disengaged
  - Schedule generation UI in character profile Library
  - Plaintext schedule format (more reliable than JSON, fewer tokens)
- **Dual LLM system**: Decision engine + content generator architecture
  - Small LLM makes behavioral decisions (reactions, mood changes)
  - Large LLM generates character responses
  - Separate configurable settings for each in Profile (tabs: Content LLM / Decision LLM)
- **Reaction system**: Emoji reactions overlay on user messages
  - Rare (1 in 5 messages or less), only on emotionally significant messages
  - Positioned absolutely on user message bubbles (-bottom-2, -right-2)
  - Stored in messages table, displayed via lookahead (check next message)
- **File logging**: All console output logged to `backend/logs/server.log`
  - Auto-clears on server restart
  - Intercepts console.log/error/warn at startup
  - Clean output (removed SQL spam, excessive response details)
- **Message editing/deletion**: Edit message content, delete message + all after it
- **Regenerate responses**: Generate new AI response for current conversation state
- **Context window system**: Token counting with gpt-tokenizer, smart message trimming
- **Multi-message display**: AI responses split by newlines with progressive 800ms delays
- **Typing indicator**: Changed from animated dots to text-based with random delay
- **Message animations**: Slide-up only on new messages (prevents re-animation on render)
- **Sidebar previews**: Shows last message text instead of "Start chatting..."
- **Chat UI cleanup**: Removed character avatars from message bubbles
- LLM Settings now includes adjustable context window (1K-200K tokens)

## Environment Variables

Backend `.env`:
```
PORT=3000
FRONTEND_URL=http://localhost:5173
JWT_SECRET=<random-secret>
OPENROUTER_API_KEY=<your-key>
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
```

No frontend env vars needed (API URL hardcoded to localhost:3000).
