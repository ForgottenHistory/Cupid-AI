# Cupid AI - Technical Context

> Current implementation details and architecture. See PLAN.md for planned features.

## Architecture Overview

**Stack**: React + Vite frontend, Node.js + Express backend, SQLite database
**AI Provider**: OpenRouter (configurable per-user LLM settings)
**Storage**: IndexedDB (frontend) + SQLite (backend)

## Key Implementation Details

### Frontend Structure
- **Main Layout**: `MainLayout.jsx` - Sidebar with matches, unread indicators, nav
- **Core Pages**:
  - `Home.jsx` - Tinder-style swipe interface
  - `Chat.jsx` - Individual chat conversations
  - `Library.jsx` - Character management
  - `Profile.jsx` - User settings and LLM config
- **Services**:
  - `characterService.js` - IndexedDB operations for characters
  - `chatService.js` - Backend API calls for conversations/messages
  - `api.js` - Axios instance with auth token injection

### Backend Structure
- **Routes**: `auth.js`, `users.js`, `characters.js`, `chat.js`
- **Services**:
  - `aiService.js` - OpenRouter integration, system prompts, token counting/trimming, decision engine
  - `engagementService.js` - Character state tracking, time-based engagement durations, cooldown system
- **Utils**: `logger.js` - File logging with console intercept (auto-clears on restart)
- **Database**: `database.js` - better-sqlite3 with auto-migrations
- **Auth**: JWT tokens, `authenticateToken` middleware

## Important Patterns

### Character Data Flow
1. PNG upload → Parse v2 card → Store in IndexedDB (frontend)
2. Like character → Sync to backend SQLite (user_id + character_id)
3. Dating profile generated on-demand via OpenRouter (cached in cardData)

### Chat System
- Conversations created on first message
- `unread_count` tracks unread messages per conversation
- Messages marked as read when chat is opened
- AI responses use character description + dating profile + system prompt
- Component unmount tracking prevents stale state updates
- **Context window management**: Token-based trimming keeps recent messages, drops old ones
- **Multi-message system**: Newlines split into separate bubbles with 800ms delays
- **Typing indicator**: Text-based "{name} is typing..." with random 500-2000ms delay
- **Message animations**: Slide-up on new messages only (tracked via `newMessageIds` Set)
- **Dual LLM system**: Decision LLM makes behavioral decisions (reactions), Content LLM generates responses
- **Reaction system**: Emoji reactions appear rarely (1 in 5 messages) on emotionally significant user messages
- **Message editing/deletion**: Users can edit or delete messages, delete-from removes message + all after it
- **Regenerate**: Users can regenerate the last AI response
- **Schedule system**: Characters have weekly schedules (online/away/busy/offline) generated by LLM
- **Status display**: Chat header shows real-time status with colored badges and activity text
- **Time-based engagement**: Characters stay engaged for realistic durations based on status, then depart naturally
- **AI reply suggestions**: Three-button UI generates suggested user replies (serious, sarcastic, flirty styles)

### Unread Notifications
- Backend increments `unread_count` on AI responses
- `markAsRead` endpoint resets count when user views chat
- `characterUpdated` custom event triggers sidebar refresh
- First messages generated at match time (50/50 chance)

## Critical Bugs Fixed

### Race Condition with Chat Navigation
**Issue**: Sending message in Chat A, navigating to Chat B before response → Chat B shows Chat A's messages
**Fix**: Capture `characterId` at request start, check `isMountedRef.current` before state updates

### Double First Message Generation
**Issue**: First message generated twice due to React strict mode + useEffect
**Fix**: Moved generation to swipe action (Home.jsx), removed from Chat.jsx load

### Unread Count Not Clearing
**Issue**: Messages marked as read even when user navigated away
**Fix**: Added `isMountedRef` to track if user is still viewing that chat

## Database Schema Notes

### Key Tables
- `users` - LLM settings (model, temperature, max_tokens, context_window, etc.) + Decision LLM settings
- `conversations` - Links user + character, tracks `unread_count`, `last_message`
- `messages` - role ('user'|'assistant'), content, timestamps, reaction (emoji or null)
- `characters` - Synced from IndexedDB, stores full card_data JSON, schedule_data, schedule_generated_at
- `character_states` - Per user-character engagement tracking (status, engagement_state, engagement_started_at, departed_status, last_check_time)

### Migrations
Auto-run on startup via `database.js`:
- Adds LLM settings columns if missing
- Adds `unread_count` column if missing
- Adds `llm_context_window` column (default 4000 tokens)
- Adds Decision LLM settings columns (model, temperature, max_tokens, context_window with lower defaults)
- Adds `reaction` column to messages table
- Adds `schedule_data` and `schedule_generated_at` columns to characters table
- Creates `character_states` table for engagement tracking
- Adds `engagement_started_at` and `departed_status` columns to character_states table

## AI System

### Dual LLM Architecture
- **Decision Engine** (small LLM): Analyzes conversation context and makes behavioral decisions
  - Determines if character should react with emoji (rare, 1 in 5 messages or less)
  - Decides if character should unmatch (extremely rare, only for inappropriate behavior)
  - Returns: `{ reaction: "emoji"|null, shouldRespond: boolean, shouldUnmatch: boolean }`
  - Engagement duration handled programmatically by time-based system
  - Uses separate user-configurable settings (lower temperature, fewer tokens)
  - Located in `aiService.js` `makeDecision()`
- **Content Generator** (large LLM): Creates actual character responses
  - Uses full character prompt with dating app style enforcement
  - Token-based context window management
  - Located in `aiService.js` `createChatCompletion()`

### Content LLM System Prompt Rules
Located in `aiService.js` `buildSystemPrompt()`:
- Explicitly forbids roleplay formatting (*actions*, "dialogue")
- Enforces dating app text message style
- Uses character description + scenario + system_prompt fields
- Dating profile injected into character data if available
- When `isDeparting` flag is true, prompts for natural departure message using schedule context

### Message Flow
1. User sends message → saved to DB
2. Check character's schedule → determine current status (online/away/busy/offline)
3. Check if character is on cooldown (waiting for status change) → if yes, no response
4. If status changed from departed status → clear cooldown
5. If disengaged: 70% chance to engage and respond, 30% no response
6. Apply fast response delay (~1 second with variance)
7. Check if engagement duration expired → if yes, set `isDeparting` flag
8. Decision LLM analyzes context → decides reaction and unmatch
9. **If unmatch decision**: Character deleted from backend, conversation removed, unmatch modal shown
10. Content LLM generates response (with departure context if flagged)
11. If departing → mark character as departed, start cooldown until status changes
12. Response saved with reaction → displayed with reaction badge on user's message

### Time-Based Engagement System
Located in `engagementService.js`:
- **Response delays**: All responses ~0.5-2 seconds (fast, realistic texting speed)
- **Engagement durations** (characters stay engaged for realistic time periods):
  - **Online**: Unlimited (stays engaged as long as conversation flows)
  - **Away**: 30-60 minutes → then sends natural departure message
  - **Busy**: 15-30 minutes → then sends natural departure message
  - **Offline**: No response
- **Departure behavior**: When duration expires, Content LLM generates natural "gtg" message using schedule context
- **Cooldown system**: After departing, character won't respond until schedule status changes
- **70% engagement probability**: On first message when disengaged (30% chance of no response)
- **State tracking**: `character_states` table tracks `engagement_started_at`, `departed_status` per user-character pair

## Development Commands

```bash
# Frontend
cd frontend && npm run dev  # Port 5173

# Backend
cd backend && npm start     # Port 3000 (nodemon auto-restart)
```

## Known Quirks

- Character names default to "Character" when syncing to backend (cosmetic)
- Dating profile generation uses fixed prompt in `characters.js`
- Image parsing happens client-side, backend only stores metadata
- Swipe undo limited to last action (no full history)
- Match animation doesn't auto-dismiss (user must click)

## Recent Changes

- **Time-based engagement overhaul** ✅
  - Replaced arbitrary message-count disengagement with realistic time-based durations
  - Online: unlimited engagement, Away: 30-60min, Busy: 15-30min
  - Natural departure messages generated by Content LLM using schedule context
  - Cooldown system blocks responses until character's status changes
  - Simplified all response delays to ~1 second (removed status-based delays)
  - Removed `continueEngagement` from Decision LLM (now handled programmatically)
  - Characters stay engaged during good conversations instead of dropping out arbitrarily
- **AI reply suggestions** ✅
  - Three-button UI in chat input: Serious, Sarcastic, Flirty
  - Generates suggested user replies based on conversation context
  - Uses separate API endpoint: `/api/chat/conversations/:characterId/suggest-reply`
  - Helps users craft better responses in different tones
- **Character-initiated unmatch** ✅
  - Decision Engine can decide to unmatch (extremely rare, only for inappropriate user behavior)
  - Character deleted from backend, conversation removed, user notified via WebSocket
  - Animated unmatch modal (similar to match modal but with broken hearts and red theme)
  - Modal shows character name, grayscale image, and "Back to Home" button
  - Debug function `debugUnmatch()` available in browser console for testing
  - Located in: `messageProcessor.js` (backend), `UnmatchModal.jsx` (frontend), `useChatWebSocket.js` (WebSocket handler)
- **Phase 2: Schedule & Engagement System** ✅
  - LLM-generated weekly schedules (online/away/busy/offline status blocks)
  - Real-time status calculation from schedule + current time
  - Status display in chat header with colored badges
  - Engagement window system: Initial delays based on status, then 2-5 message bursts with fast responses (5-15s)
  - character_states table tracks engagement per user-character pair
  - 70% engagement probability on first message when disengaged
  - Schedule generation UI in character profile Library
  - Plaintext schedule format (more reliable than JSON, fewer tokens)
- **Dual LLM system**: Decision engine + content generator architecture
  - Small LLM makes behavioral decisions (reactions, mood changes, engagement, unmatch)
  - Large LLM generates character responses
  - Separate configurable settings for each in Profile (tabs: Content LLM / Decision LLM)
- **Reaction system**: Emoji reactions overlay on user messages
  - Rare (1 in 5 messages or less), only on emotionally significant messages
  - Positioned absolutely on user message bubbles (-bottom-2, -right-2)
  - Stored in messages table, displayed via lookahead (check next message)
- **File logging**: All console output logged to `backend/logs/server.log`
  - Auto-clears on server restart
  - Intercepts console.log/error/warn at startup
  - Clean output (removed SQL spam, excessive response details)
- **Message editing/deletion**: Edit message content, delete message + all after it
- **Regenerate responses**: Generate new AI response for current conversation state
- **Context window system**: Token counting with gpt-tokenizer, smart message trimming
- **Multi-message display**: AI responses split by newlines with progressive 800ms delays
- **Typing indicator**: Changed from animated dots to text-based with random delay
- **Message animations**: Slide-up only on new messages (prevents re-animation on render)
- **Sidebar previews**: Shows last message text instead of "Start chatting..."
- **Chat UI cleanup**: Removed character avatars from message bubbles
- LLM Settings now includes adjustable context window (1K-200K tokens)

## Environment Variables

Backend `.env`:
```
PORT=3000
FRONTEND_URL=http://localhost:5173
JWT_SECRET=<random-secret>
OPENROUTER_API_KEY=<your-key>
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
```

No frontend env vars needed (API URL hardcoded to localhost:3000).
